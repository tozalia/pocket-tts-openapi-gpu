# Pocket TTS OpenAPI - GPU Enhanced Edition

Fast, local, OpenAI-compatible TTS server with **GPU acceleration**, **multi-threading**, and **word-level alignment** for Remotion.

> Forked from [IceFog72/pocket-tts-openapi](https://github.com/IceFog72/pocket-tts-openapi) with GPU optimizations adapted from [groxaxo/Qwen3-TTS-Openai-Fastapi](https://github.com/groxaxo/Qwen3-TTS-Openai-Fastapi).

## Features

| Feature | Description |
|---------|-------------|
| üöÄ **GPU Acceleration** | Auto-detects CUDA, moves model to GPU |
| ‚ö° **torch.compile()** | 20-30% speedup via kernel fusion |
| üî• **TF32 Precision** | 3-5x matmul speedup on Ampere+ GPUs |
| üßµ **Multi-threading** | Semaphore-based concurrent inference |
| üì¶ **Ungated Model** | No HuggingFace login required |
| üíæ **Audio Caching** | Instant response for repeated phrases |
| üé≠ **Voice Cloning** | Drop WAV files in `voices/` folder |
| üé¨ **Remotion Integration** | Word-level timestamps for captions |

## Quick Start

```bash
# Clone
git clone https://github.com/YOUR_REPO/pocket-tts-openapi-gpu
cd pocket-tts-openapi-gpu

# Install with GPU support
chmod +x install_gpu.sh start_gpu.sh
./install_gpu.sh

# Start server
./start_gpu.sh
```

Server runs at `http://localhost:8001`

## Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/audio/speech` | POST | Generate speech (OpenAI-compatible) |
| `/v1/audio/speech-with-alignment` | POST | **TTS + word timestamps** (Remotion) |
| `/v1/audio/align` | POST | Align existing audio with transcript |
| `/v1/voices` | GET | List voices |
| `/v1/models` | GET | List models |
| `/health` | GET | Health + metrics |
| `/docs` | GET | Swagger UI |

## Unified TTS + Alignment (Remotion)

Generate audio and get word-level timestamps in a single request:

```python
import requests
import json
import base64

response = requests.post("http://localhost:8001/v1/audio/speech-with-alignment", json={
    "input": "Hello world, this is a test.",
    "voice": "alloy",  # or path to WAV for voice cloning
    "fps": 30,  # for frame calculations
})

data = response.json()

# Save audio
with open("narration.wav", "wb") as f:
    f.write(base64.b64decode(data["audio_base64"]))

# Save Remotion-compatible captions
with open("src/captions.json", "w") as f:
    json.dump(data["captions"], f, indent=2)

# Captions format:
# [
#   {"text": "Hello", "startMs": 0, "endMs": 250, "startFrame": 0, "endFrame": 7},
#   {"text": "world,", "startMs": 250, "endMs": 520, "startFrame": 7, "endFrame": 15},
#   ...
# ]
```

## Remotion Integration

Use the captions directly with Remotion's Caption component:

```tsx
// src/captions.json generated by speech-with-alignment endpoint
import captions from './captions.json';

export const TikTokCaptions: React.FC = () => {
  const frame = useCurrentFrame();
  const { fps } = useVideoConfig();
  
  const currentCaption = captions.find(c => 
    frame >= c.startFrame && frame <= c.endFrame
  );
  
  return currentCaption ? <div>{currentCaption.text}</div> : null;
};
```

## Configuration

| Environment Variable | Default | Description |
|---------------------|---------|-------------|
| `POCKET_TTS_DEVICE` | `auto` | `auto`, `cuda`, or `cpu` |
| `POCKET_TTS_WORKERS` | `4` | Max concurrent requests |
| `POCKET_TTS_COMPILE` | `true` | Enable torch.compile() |
| `POCKET_TTS_TF32` | `true` | TF32 for Ampere+ GPUs |
| `POCKET_TTS_CUDNN_BENCH` | `true` | cuDNN benchmark mode |
| `POCKET_TTS_MODEL` | `Verylicious/pocket-tts-ungated` | HuggingFace model |

## Alignment Methods

The API uses two alignment methods:

1. **Montreal Forced Aligner (MFA)** - Precise phoneme-level alignment
   - Install: `pip install pyfoal` + `conda install -c conda-forge montreal-forced-aligner`
   
2. **Proportional Estimation** (fallback) - Character-based timing estimation
   - Works out of the box, no extra dependencies
   - Good enough for most use cases

Check `/health` endpoint to see which method is active.

## Standard OpenAI Usage

```python
from openai import OpenAI

client = OpenAI(base_url="http://localhost:8001/v1", api_key="not-needed")

response = client.audio.speech.create(
    model="tts-1",
    voice="alloy",
    input="Hello world!",
    response_format="mp3"
)

with open("output.mp3", "wb") as f:
    f.write(response.read())
```

## Voice Cloning

Drop `.wav` files into the `voices/` directory:

```bash
voices/
‚îú‚îÄ‚îÄ my_voice.wav    # Use voice="my_voice"
‚îî‚îÄ‚îÄ narrator.wav    # Use voice="narrator"
```

## Performance Notes

‚ö†Ô∏è **First 2-3 requests will be slower** due to torch.compile() warmup.

After warmup:
- **GPU**: ~2-3x realtime on RTX 2050/3060
- **CPU**: ~1.5x realtime on modern CPUs

## License

MIT - Same as original pocket-tts-openapi
